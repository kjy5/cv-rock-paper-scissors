{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the RPS model\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kjy5/cv-rock-paper-scissors/blob/main/scripts/train.ipynb)\n",
    "\n",
    "# TEMP: resources:\n",
    "- For transfer learning see [transfer learning](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)\n",
    "\n",
    "- For saving/loading: [save load run](https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html)\n",
    "\n",
    "## Setup\n",
    "### 1. Download data\n",
    "Only need to run this once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=11IkCeaEsjysSaWgMEI3SwkSzJ1Tmxz1i&confirm=t' -O data.zip\n",
    "!unzip data.zip\n",
    "!rm -rf data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "import cv2 as cv\n",
    "from PIL import Image\n",
    "import os\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3. Set up device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = torch.device(\"cpu\")\n",
    "# Use a CUDA GPU if possible (Apple Silicon MPS backend technically works, but is confusingly slow)\n",
    "if torch.cuda.is_available():\n",
    "    d = torch.device(\"cuda:0\")\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Loading data and model\n",
    "### Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"data\"\n",
    "CLASSES = [\"rock\", \"paper\", \"scissors\", \"clutter\"]\n",
    "\n",
    "TRAIN_PHASE = \"train\"\n",
    "VAL_PHASE = \"val\"\n",
    "\n",
    "CLUTTER_IMAGE_PREFIX = \"test_\"\n",
    "CLUTTER_COUNT = 10000\n",
    "\n",
    "NUM_FRAMES = 120\n",
    "TRAIN_VAL_SPLIT = NUM_FRAMES * 0.8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Define transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize(224),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "preprocess = {\n",
    "    TRAIN_PHASE: transforms.Compose(\n",
    "        [\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    ),\n",
    "    VAL_PHASE: transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    ),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define dataloaders\n",
    "dataloaders = {\n",
    "    TRAIN_PHASE: {},\n",
    "    VAL_PHASE: {},\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clutter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 240 random clutter images\n",
    "clutter_frame_ids = random.sample(range(CLUTTER_COUNT), NUM_FRAMES * 2)\n",
    "FRAME_SPLIT = int(TRAIN_VAL_SPLIT * 2)\n",
    "\n",
    "for phase, preprocessor in preprocess.items():\n",
    "    # Configure image cache\n",
    "    data = deque()\n",
    "    ids = (\n",
    "        clutter_frame_ids[:FRAME_SPLIT]\n",
    "        if phase == TRAIN_PHASE\n",
    "        else clutter_frame_ids[FRAME_SPLIT:]\n",
    "    )\n",
    "\n",
    "    # Load and preprocess images\n",
    "    for img_id in ids:\n",
    "        cur_image = Image.open(\n",
    "            os.path.join(DATA_DIR, \"clutter\", f\"{CLUTTER_IMAGE_PREFIX}{img_id}.JPEG\")\n",
    "        )\n",
    "        # Ensure using RGB\n",
    "        if cur_image.mode != \"RGB\":\n",
    "            cur_image = cur_image.convert(\"RGB\")\n",
    "        data.append(preprocessor(cur_image))\n",
    "        cur_image.close()\n",
    "\n",
    "    # Save to dataloader\n",
    "    dataloaders[phase][\"clutter\"] = torch.utils.data.DataLoader(\n",
    "        data, batch_size=4, shuffle=True, num_workers=4\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RPS data (videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in a pretrained model\n",
    "model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "model.to(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load presaved model\n",
    "model = torch.load(\"../model.pth\")\n",
    "model.to(d)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks:\n",
    "Read whole training videos into memory\n",
    "\n",
    "Read clutter dataset into memory\n",
    "\n",
    "## Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Load in training data\n",
    "#### 1. Rock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "video_file = cv.VideoCapture(\"../data/rock/rock.mp4\")\n",
    "\n",
    "# Count frames\n",
    "num_frames = 0\n",
    "while True:\n",
    "    ret, frame = video_file.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    num_frames += 1\n",
    "\n",
    "frames = torch.empty(num_frames, 3, 224, 224)\n",
    "frames_index = 0\n",
    "video_file = cv.VideoCapture(\"../data/rock/rock.mp4\")\n",
    "while True:\n",
    "    ret, frame = video_file.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Rearrange the channels\n",
    "    image = frame[:, :, [2, 1, 0]]\n",
    "\n",
    "    # Run preprocess\n",
    "    image_tensor = preprocess(image).to(d)\n",
    "\n",
    "    # Append to processed frames\n",
    "    frames[frames_index] = image_tensor\n",
    "\n",
    "print(f'Loaded {num_frames} frames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model, \"../model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20c45cb1bf71dfa8979fa82a78b4fb18d0aae9298832ad48008189d98f375aaa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
