{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the RPS model\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kjy5/cv-rock-paper-scissors/blob/main/scripts/train.ipynb)\n",
    "\n",
    "# TEMP: resources:\n",
    "- For transfer learning see [transfer learning](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)\n",
    "\n",
    "- For saving/loading: [save load run](https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html)\n",
    "\n",
    "## Setup\n",
    "### 1. Download data\n",
    "Only need to run this once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=11IkCeaEsjysSaWgMEI3SwkSzJ1Tmxz1i&confirm=t' -O data.zip\n",
    "!unzip data.zip\n",
    "!rm -rf data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "import cv2 as cv\n",
    "from PIL import Image\n",
    "import os\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3. Set up device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = torch.device(\"cpu\")\n",
    "# Use a CUDA GPU if possible (Apple Silicon MPS backend technically works, but is confusingly slow)\n",
    "if torch.cuda.is_available():\n",
    "    d = torch.device(\"cuda:0\")\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Loading data and model\n",
    "### Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"data\"\n",
    "CLASSES = [\"rock\", \"paper\", \"scissors\", \"clutter\"]\n",
    "VIDEOS_PER_CLASS = 2\n",
    "\n",
    "TRAIN_PHASE = \"train\"\n",
    "VAL_PHASE = \"val\"\n",
    "\n",
    "CLUTTER_IMAGE_PREFIX = \"test_\"\n",
    "CLUTTER_COUNT = 10000\n",
    "\n",
    "NUM_FRAMES = 120\n",
    "TRAIN_VAL_SPLIT = int(NUM_FRAMES * 0.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Define transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize(224),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "transformers = {\n",
    "    TRAIN_PHASE: transforms.Compose(\n",
    "        [\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    ),\n",
    "    VAL_PHASE: transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    ),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define dataloaders\n",
    "dataloaders = {\n",
    "    TRAIN_PHASE: {},\n",
    "    VAL_PHASE: {},\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clutter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 240 random clutter images\n",
    "clutter_frame_ids = random.sample(range(CLUTTER_COUNT), NUM_FRAMES * 2)\n",
    "FRAME_SPLIT = int(TRAIN_VAL_SPLIT * 2)\n",
    "\n",
    "for phase, transformer in transformers.items():\n",
    "    # Configure image cache\n",
    "    data = deque()\n",
    "    ids = (\n",
    "        clutter_frame_ids[:FRAME_SPLIT]\n",
    "        if phase == TRAIN_PHASE\n",
    "        else clutter_frame_ids[FRAME_SPLIT:]\n",
    "    )\n",
    "\n",
    "    # Load and preprocess images\n",
    "    for img_id in ids:\n",
    "        cur_image = Image.open(\n",
    "            os.path.join(DATA_DIR, \"clutter\", f\"{CLUTTER_IMAGE_PREFIX}{img_id}.JPEG\")\n",
    "        )\n",
    "        # Ensure using RGB\n",
    "        if cur_image.mode != \"RGB\":\n",
    "            cur_image = cur_image.convert(\"RGB\")\n",
    "        data.append(transformer(cur_image))\n",
    "        cur_image.close()\n",
    "\n",
    "    # Save to dataloader\n",
    "    dataloaders[phase][\"clutter\"] = torch.utils.data.DataLoader(\n",
    "        data, batch_size=4, shuffle=True, num_workers=4\n",
    "    )\n",
    "# Cleanup\n",
    "del data, ids, clutter_frame_ids\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RPS data (videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "pic should be PIL Image or ndarray. Got <class 'torch.Tensor'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m random\u001b[39m.\u001b[39mshuffle(frames)\n\u001b[1;32m     28\u001b[0m frames \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(frames)\n\u001b[0;32m---> 29\u001b[0m \u001b[39mprint\u001b[39m(transformers[TRAIN_PHASE](torch\u001b[39m.\u001b[39;49mtensor(frames[\u001b[39m0\u001b[39;49m]))\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     31\u001b[0m \u001b[39m# Split into train and val and save to dataloader\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39mfor\u001b[39;00m phase, transformer \u001b[39min\u001b[39;00m transformers\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/envs/cv-rps/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/envs/cv-rps/lib/python3.10/site-packages/torchvision/transforms/transforms.py:135\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[1;32m    128\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/envs/cv-rps/lib/python3.10/site-packages/torchvision/transforms/functional.py:137\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    135\u001b[0m     _log_api_usage_once(to_tensor)\n\u001b[1;32m    136\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (F_pil\u001b[39m.\u001b[39m_is_pil_image(pic) \u001b[39mor\u001b[39;00m _is_numpy(pic)):\n\u001b[0;32m--> 137\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpic should be PIL Image or ndarray. Got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(pic)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[39mif\u001b[39;00m _is_numpy(pic) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_numpy_image(pic):\n\u001b[1;32m    140\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpic should be 2/3 dimensional. Got \u001b[39m\u001b[39m{\u001b[39;00mpic\u001b[39m.\u001b[39mndim\u001b[39m}\u001b[39;00m\u001b[39m dimensions.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: pic should be PIL Image or ndarray. Got <class 'torch.Tensor'>"
     ]
    }
   ],
   "source": [
    "# Loop through each RPS class\n",
    "for rps_class in CLASSES[:-1]:\n",
    "    dir = os.path.join(DATA_DIR, rps_class)\n",
    "\n",
    "    # Make frame cache\n",
    "    frames = deque()\n",
    "\n",
    "    # Loop through each video\n",
    "    with os.scandir(dir) as it:\n",
    "        for entry in it:\n",
    "            if entry.name.endswith(\".mp4\") and entry.is_file():\n",
    "                # Load video\n",
    "                cap = cv.VideoCapture(os.path.join(dir, entry.name))\n",
    "\n",
    "                # Load frames\n",
    "                while True:\n",
    "                    ret, frame = cap.read()\n",
    "                    if ret:\n",
    "                        # Switch channels\n",
    "                        frame = frame[:, :, [2, 1, 0]]\n",
    "                        frames.append(frame)\n",
    "                    else:\n",
    "                        break\n",
    "                cap.release()\n",
    "\n",
    "    # Shuffle frames and convert to array\n",
    "    random.shuffle(frames)\n",
    "    frames = list(frames)\n",
    "\n",
    "    # Split into train and val and save to dataloader\n",
    "    for phase, transformer in transformers.items():\n",
    "        frame_segment = (\n",
    "            frames[:TRAIN_VAL_SPLIT] if phase == TRAIN_PHASE else frames[TRAIN_VAL_SPLIT:]\n",
    "        )\n",
    "        dataloaders[phase][rps_class] = torch.utils.data.DataLoader(\n",
    "            [transformer(img) for img in frame_segment],\n",
    "            batch_size=4,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in a pretrained model\n",
    "model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "model.to(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load presaved model\n",
    "model = torch.load(\"../model.pth\")\n",
    "model.to(d)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks:\n",
    "Read whole training videos into memory\n",
    "\n",
    "Read clutter dataset into memory\n",
    "\n",
    "## Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Load in training data\n",
    "#### 1. Rock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "video_file = cv.VideoCapture(\"../data/rock/rock.mp4\")\n",
    "\n",
    "# Count frames\n",
    "num_frames = 0\n",
    "while True:\n",
    "    ret, frame = video_file.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    num_frames += 1\n",
    "\n",
    "frames = torch.empty(num_frames, 3, 224, 224)\n",
    "frames_index = 0\n",
    "video_file = cv.VideoCapture(\"../data/rock/rock.mp4\")\n",
    "while True:\n",
    "    ret, frame = video_file.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Rearrange the channels\n",
    "    image = frame[:, :, [2, 1, 0]]\n",
    "\n",
    "    # Run preprocess\n",
    "    image_tensor = preprocess(image).to(d)\n",
    "\n",
    "    # Append to processed frames\n",
    "    frames[frames_index] = image_tensor\n",
    "\n",
    "print(f'Loaded {num_frames} frames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model, \"../model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20c45cb1bf71dfa8979fa82a78b4fb18d0aae9298832ad48008189d98f375aaa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
