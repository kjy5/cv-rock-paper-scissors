{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the RPS model\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kjy5/cv-rock-paper-scissors/blob/main/scripts/train.ipynb)\n",
    "\n",
    "# TEMP: resources:\n",
    "- For transfer learning see [transfer learning](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)\n",
    "\n",
    "- For saving/loading: [save load run](https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html)\n",
    "\n",
    "## Setup\n",
    "### 1. Download data\n",
    "Only need to run this once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=11IkCeaEsjysSaWgMEI3SwkSzJ1Tmxz1i&confirm=t' -O data.zip\n",
    "!unzip data.zip\n",
    "!rm -rf data.zip"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Import libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "import cv2 as cv\n",
    "import os\n",
    "from collections import deque\n",
    "from PIL import Image\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Set up device"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "d = torch.device(\"cpu\")\n",
    "# Use a CUDA GPU if possible (Apple Silicon MPS backend technically works, but is confusingly slow)\n",
    "if torch.cuda.is_available():\n",
    "    d = torch.device(\"cuda:0\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading data and model\n",
    "### Define constants"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DATA_DIR = \"data\"\n",
    "CLASSES = [\"rock\", \"paper\", \"scissors\", \"clutter\"]\n",
    "CLUTTER_IMAGE_PREFIX = \"test_\"\n",
    "CLUTTER_COUNT = 10000\n",
    "NUM_FRAMES = 120"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define transformers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(224),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Define dataloaders\n",
    "train_dataloaders = {}\n",
    "val_dataloaders = {}\n",
    "\n",
    "# Load 240 random clutter images\n",
    "clutter_frame_ids = random.sample(range(CLUTTER_COUNT), NUM_FRAMES * 2)\n",
    "FRAME_SPLIT = int(NUM_FRAMES * 1.6)\n",
    "\n",
    "# Load train images (80%)\n",
    "train_data = deque()\n",
    "for img_id in clutter_frame_ids[:FRAME_SPLIT]:\n",
    "    cur_image = Image.open(os.path.join(DATA_DIR, \"clutter\", f\"{CLUTTER_IMAGE_PREFIX}{img_id}.JPEG\"))\n",
    "    # Ensure using RGB\n",
    "    if cur_image.mode != \"RGB\":\n",
    "        cur_image = cur_image.convert(\"RGB\")\n",
    "    train_data.append(train_transform(cur_image))\n",
    "    cur_image.close()\n",
    "train_dataloaders[\"clutter\"] = torch.utils.data.DataLoader(train_data, batch_size=4, shuffle=True, num_workers=4)\n",
    "\n",
    "# Load validation images (20%)\n",
    "val_data = deque()\n",
    "for img_id in clutter_frame_ids[FRAME_SPLIT:]:\n",
    "    cur_image = Image.open(os.path.join(DATA_DIR, \"clutter\", f\"{CLUTTER_IMAGE_PREFIX}{img_id}.JPEG\"))\n",
    "    # Ensure using RGB\n",
    "    if cur_image.mode != \"RGB\":\n",
    "        cur_image = cur_image.convert(\"RGB\")\n",
    "    val_data.append(val_transform(cur_image))\n",
    "    cur_image.close()\n",
    "val_dataloaders[\"clutter\"] = torch.utils.data.DataLoader(val_data, batch_size=4, shuffle=True, num_workers=4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in a pretrained model\n",
    "model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "model.to(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load presaved model\n",
    "model = torch.load(\"../model.pth\")\n",
    "model.to(d)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks:\n",
    "Read whole training videos into memory\n",
    "\n",
    "Read clutter dataset into memory\n",
    "\n",
    "## Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load in training data\n",
    "#### 1. Rock"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "video_file = cv.VideoCapture(\"../data/rock/rock.mp4\")\n",
    "\n",
    "# Count frames\n",
    "num_frames = 0\n",
    "while True:\n",
    "    ret, frame = video_file.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    num_frames += 1\n",
    "\n",
    "frames = torch.empty(num_frames, 3, 224, 224)\n",
    "frames_index = 0\n",
    "video_file = cv.VideoCapture(\"../data/rock/rock.mp4\")\n",
    "while True:\n",
    "    ret, frame = video_file.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Rearrange the channels\n",
    "    image = frame[:, :, [2, 1, 0]]\n",
    "\n",
    "    # Run preprocess\n",
    "    image_tensor = preprocess(image).to(d)\n",
    "\n",
    "    # Append to processed frames\n",
    "    frames[frames_index] = image_tensor\n",
    "\n",
    "print(f'Loaded {num_frames} frames')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model, \"../model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20c45cb1bf71dfa8979fa82a78b4fb18d0aae9298832ad48008189d98f375aaa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
